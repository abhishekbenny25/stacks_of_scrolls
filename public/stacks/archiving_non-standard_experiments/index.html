<!doctype html><html lang=en dir=auto data-theme=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Archiving Non-Standard Experiments (Entropy-Moth Proofing) | Stacks of Scrolls</title><meta name=keywords content><meta name=description content="A practical guide to building a knowledgebase that stands the test of time"><meta name=author content="Abhishek Benny"><link rel=canonical href=http://localhost:1313/stacks/archiving_non-standard_experiments/><link crossorigin=anonymous href=/assets/css/stylesheet.19eedea31a05c8a353e7093ff09c56c81ab1632019d50c572b722424f4f235a9.css integrity="sha256-Ge7eoxoFyKNT5wk/8JxWyBqxYyAZ1QxXK3IkJPTyNak=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/favicon-32x32.png><link rel=apple-touch-icon href=http://localhost:1313/apple-touch-icon.png><link rel=mask-icon href=http://localhost:1313/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/stacks/archiving_non-standard_experiments/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-VBYP6TM1D3"></script><script>var doNotTrack=!1,dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes";if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-VBYP6TM1D3")}</script><meta property="og:url" content="http://localhost:1313/stacks/archiving_non-standard_experiments/"><meta property="og:site_name" content="Stacks of Scrolls"><meta property="og:title" content="Archiving Non-Standard Experiments (Entropy-Moth Proofing)"><meta property="og:description" content="A practical guide to building a knowledgebase that stands the test of time"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="stacks"><meta property="article:published_time" content="2026-01-27T11:48:40+04:00"><meta property="article:modified_time" content="2026-01-27T11:48:40+04:00"><meta property="og:image" content="http://localhost:1313/stacks/archiving_non-standard_experiments/images/covers/archiving_non-standard_experiments.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/stacks/archiving_non-standard_experiments/images/covers/archiving_non-standard_experiments.png"><meta name=twitter:title content="Archiving Non-Standard Experiments (Entropy-Moth Proofing)"><meta name=twitter:description content="A practical guide to building a knowledgebase that stands the test of time"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"The Stacks üìú","item":"http://localhost:1313/stacks/"},{"@type":"ListItem","position":2,"name":"Archiving Non-Standard Experiments (Entropy-Moth Proofing)","item":"http://localhost:1313/stacks/archiving_non-standard_experiments/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Archiving Non-Standard Experiments (Entropy-Moth Proofing)","name":"Archiving Non-Standard Experiments (Entropy-Moth Proofing)","description":"A practical guide to building a knowledgebase that stands the test of time","keywords":[],"articleBody":"In any R\u0026D or continuous improvement environment, progress depends on knowledge transfer. Often, a lot of the most valuable lessons live in people‚Äôs heads: the ‚Äúwe tried that in 2014 ‚Ä¶‚Äù experiments, the one-off protocols, the messy edge cases and details that did not make it into a formal report. Newer teams can try to survey the literature and internal documentation, but without a guide, it‚Äôs hard to know what exists, let alone where it‚Äôs stored, what to trust, and under what circumstances the data is valid.\nWhen one of these human libraries leaves, the organization does not just lose their experience, it also loses the retrievability of the data. You‚Äôre stranded, repeating work, missing prior constraints, losing context and spending large amounts of time and money rebuilding it from scratch. And that‚Äôs when the entropy moths move in, quietly chewing through whatever scraps remain: half-labeled folders, orphaned spreadsheets, and undocumented assumptions, until ‚Äúwe definitely learned something here‚Äù becomes ‚Äúwe have no idea what this means.‚Äù\nThat‚Äôs why a well-maintained, searchable knowledgebase isn‚Äôt a ‚Äúnice to have‚Äù. It‚Äôs vital infrastructure. The catch is that continuous improvement is inherently ad hoc. Protocols vary, outputs are not standardized, and important metadata is often scattered across OneNote, meeting minutes, and oral tradition. Archiving this is non-trivial: the system needs to be searchable and auditable without stripping away the data richness that makes it worth keeping around.\nThis was a core problem that I was solving in my previous role at Bayer, and more recently with my own projects. I recently built a system to run systematic backtests across large parameter grids, which naturally needed a similar knowledgebase. In this post, I‚Äôll show how to moth-proof your experiments against entropy by building an archive that captures minimum viable metadata, enforces a consistent storage ‚Äúshape,‚Äù and keeps an auditable thread from figure to dataset to run to protocol.\nWhy does knowledge go missing? Most archives don‚Äôt fail catastrophically; they get eroded slowly. Usually there are a few modes of failure:\nThe test exists, but the context does not (why did we perform the test?) The protocol exists, but metadata does not (what product did we test on? Who operated it? What version of automation did we do it with?) Results exist, but the interpretation does not (were the results good? Why?) Files existed on a person‚Äôs personal drive, but got deleted by a retention policy Multiple floating versions of a slide deck or report (Atest_final_FINAL_v7.pptx) Sometimes we have combinations of these scenarios, where the context for a follow up protocol is lost because of shared drive, or there are fundamental changes to methods that are lost.\nNow that the stakes are clear, let‚Äôs build an archive the entropy moths can‚Äôt eat!\nDefining the archive ‚Äì minimum viable metadata Unless we structure our thinking around protocols and testing, it is impossible to archive them.\nEvery protocol starts with a research question. It usually takes the form of: ‚ÄúHow can I modify X to get Y benefit from Z process?‚Äù Inherent to this question, we have a few of the KEY items we need to make our archive work. What we are testing? (modifying X), why are we doing it? (get y benefit), what is the context? (from Z process). While this is very simple, most testing follows this very pattern. We are already halfway there to defining the minimum viable metadata that we need to have a searchable archive. Here‚Äôs a quick reference of what this consists of.\nTable 1. Examples of minimum viable metadata and why we need them\nCategory Examples Purpose Identity Protocol ID\nTitle Owner\nDate Identifies the protocol from similar ones to reduce confusion. Linkages Related testing\nFollowups\nTags Links tests together so that finding context is easier without needing to provide redundant data. Justification Hypothesis\nBusiness case\nSuccess criteria Gives context as to why the test was performed. Inputs Materials\nParameter space Gives detail on what was tested and important context for future design of experiments. Methods Operating procedures\nEquipment¬†and¬†versions\nSampling methods\nAssays conducted Gives context on what may have changed and if the results are still valid. Knowledge Generated Conclusions\nRecommendations\nEdge cases\nKey discussion\nAnticipated issues This is what we are trying to preserve. This data is best stored in a sql database, however you can make excel or sharepoint work depending on your organization‚Äôs preexisting technology stack.\nThe square peg / round hole fallacy ‚Äì standardize the archiving, not the experiments Now that we have defined our non-negotiable data points which give our archive structure, you‚Äôre probably wondering how we are going to take all the rest of the data which usually has VERY LITTLE in common between tests and force them into the same database structure. The answer is ‚Äì you don‚Äôt.\nYou are going to standardize the envelope of the data rather than the content. What this means is you standardize how stuff is stored. Here‚Äôs an example file system based on the protocol identification.\nArchive/ ‚îú‚îÄ 20260101_testing_x_y_z/ ‚îÇ ‚îú‚îÄ 20260101_protocol.docx ‚îÇ ‚îú‚îÄ 20260101_runs.csv ‚îÇ ‚îú‚îÄ 20260101_results.pptx ‚îÇ ‚îú‚îÄ 20260101_data.xlsx ‚îÇ ‚îî‚îÄ rawdata/ ‚îÇ ‚îú‚îÄ 20260101_assay1.xlsx ‚îÇ ‚îú‚îÄ 20260101_assay2.xlsx ‚îÇ ‚îî‚îÄ 20260101_statistics.ipynb ‚îú‚îÄ 20251201_testing_a_b_c/ ‚îÇ ‚îî‚îÄ ‚Ä¶ ‚îî‚îÄ ‚Ä¶ This way, even if every test is different, the path to find the data is always the same.\nCongratulations, if you implemented the last 2 steps, you have a functional albeit basic archive. The challenge now is improving searchability, making it auditable, and getting people to use it.\nIf you want to find it, tag it ‚Äì improving searchability If you are familiar with SEO, you will know that good tagging is the cornerstone of making any content pop up in searches. The simplest system for tagging experiments is by using free tags.\nFree tags are liberating in that there is no rigidity, it is easy to implement and as your areas of study expand, you can expand the tag space without any additional development time and work on the archive system. However, in a system where you may have multiple groups or individuals with their own lexicon for the same phenomena this has the potential to spiral into being useless.\nOn the opposite end of the spectrum, if you use only controlled vocabulary (a dropdown) with rigid validation, you need more data management and development resources to grow the archive to fit the growing research needs. Once again, making the tagging system useless without the development resources.\nThis calls for a hybrid solution, where you add a few tags that are required and validated fields in the metadata database. These are tags you know will likely be associated with all tests such as process, test type, material type ‚Äì with the option of being left as Not Listed. The remainder of the tags should be in free tag form to allow expansion of the system as needed.\nDepending on the span of the knowledgebase, it may be worth reviewing experiments on a set time span (Annually or Semi-Annually) and making sure that tags align between departments and experimental groups.\nIf you can‚Äôt trace it, you can‚Äôt trust it ‚Äì design for auditability Unless you trust the data‚Äôs integrity, you cannot trust any conclusions you draw from it. The notion of auditability is making sure that the data is traceable and reproducible. This is where good data management principles come into play.\nRaw data needs to be immutable or at least dated and signed. Most lab notebooks allow for signing and dating raw data and observations. In excel forms, it is best practice to include a date and signature at the bottom of the data file.\nAll the data included in figures need to be traceable: figure -\u003e dataset -\u003e run -\u003e protocol. This allows reproduction of statistics and reuse of the data in comparison or meta studies. This is especially useful when tracking BOM (bill of materials) changes and changes in product quality ‚Äì as you can iteratively look at deviations in quality metrics with each BOM change experiment.\nAll deviations from protocol need to be recorded. Anything from as impactful as ‚ÄúWe excluded runs x-y because data from previous runs shows no issues‚Äù to ‚Äúwe used the wrong sample ID‚Äù need to be recorded. These deviations may be overlooked in the future leading to wrong or misleading analysis in meta studies.\nProtocols need to be versioned. Like deviations from the protocol, changes to the protocol need to be recorded. Ensure that the protocol placed in the archive is the one that was actually followed.\nAssay methods and analysis scripts need to be versioned. Changes to analysis scripts, statistical methods, or assay methodology needs to be documented and versioned so that there is context on the results. Not having documentation here can lead to misleading comparisons in future studies.\nThese rules are best enforced at the time of protocol archiving ‚Äì that is right after the study is concluded and results are published. If you wait, details fade and the moths start nibbling at your context.\nMaking the horse drink ‚Äì driving adoption You can bring the horse to the river but you cannot force it to drink. The most challenging part of designing any system is getting people to use it. If you followed along to this point, we are left with a very robust archive system that is very searchable and maintains good data management principles. The downside of being robust is that it is heavy on additional work.\nThe best way to reduce friction and get your research teams to use it is to add in a good user interface. This is twofold, both the search functionality and the data entry methods need to be optimized.\nArguably, the more important of the two is the search function. All of the tooling you built for tagging and metadata are useless unless your search engine can parse through the data and retrieve relevant results. This can be as simple as the search/filter tooling on Sharepoint or a quick webapp, or as complicated as using an LLM trained on the data.\nAnd now the counterargument: there won‚Äôt be any data to be searched if you can‚Äôt get your teams to input it. You need to develop a good UI that allows you to have validated dropdowns and defaults keyed in to reduce time spent entering data. This can be as simple as a quick PowerApp or Form which gives a few dropdowns and has a few defaults in there to speed up the entry process. Once again, depending on resources, you could outsource this process to an LLM, or at least the tagging.\nWORD OF CAUTION: LLMs enhance discoverability and ease of tagging; but does not substitute for good data management.\nKeeping the entropy moths out‚Ä¶ A good archive does not try to turn every experiment into a standardized form. It does something far simpler and significantly more powerful: it makes every experiment findable, traceable, and reuseable. By capturing the minimum viable metadata, enforcing a consistent storage method, and layering in a hybrid tagging system, you build a knowledgebase that survives team changes, retention policies and the slow erosion of context. Auditability closes the loop by ensuring that results can always be walked backward: from a figure to the protocol that generated them so future comparisons don‚Äôt become mythology.\nThe last challenge is the most human one: adoption. The system needs to feel like a shortcut to research, not a chore. When the search works well, data entry is fast and when teams can reliably reuse past work your archive transforms from being just documentation and starts being a compounding asset.\nMoth-proofed at last.\ngenAI declaration: The text matter of this blog post was completed without the use of generative AI tools. Cover image created using OpenAI‚Äôs image generation tool (DALL¬∑E). Prompt: Library of scrolls with a jumble of papers on table that are being eaten by psychadelic colored moths. Time distortion effect in the background. Semi-messy composition, blog cover illustration, landscape (1920x1080). - Abhishek Benny.\n","wordCount":"2002","inLanguage":"en","image":"http://localhost:1313/stacks/archiving_non-standard_experiments/images/covers/archiving_non-standard_experiments.png","datePublished":"2026-01-27T11:48:40+04:00","dateModified":"2026-01-27T11:48:40+04:00","author":{"@type":"Person","name":"Abhishek Benny"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/stacks/archiving_non-standard_experiments/"},"publisher":{"@type":"Organization","name":"Stacks of Scrolls","logo":{"@type":"ImageObject","url":"http://localhost:1313/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Stacks of Scrolls (Alt + H)">Stacks of Scrolls</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/ title="About Me"><span>About Me</span></a></li><li><a href=http://localhost:1313/stacks/ title=Blog><span>Blog</span></a></li><li><a href=http://localhost:1313/archives/ title=Archive><span>Archive</span></a></li><li><a href=http://localhost:1313/files/abhishek_benny.pdf title=CV target=_blank rel="noopener noreferrer"><span>CV</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;¬ª&nbsp;<a href=http://localhost:1313/stacks/>The Stacks üìú</a></div><h1 class="post-title entry-hint-parent">Archiving Non-Standard Experiments (Entropy-Moth Proofing)</h1><div class=post-description>A practical guide to building a knowledgebase that stands the test of time</div><div class=post-meta><span title='2026-01-27 11:48:40 +0400 +04'>January 27, 2026</span>&nbsp;¬∑&nbsp;<span>10 min</span>&nbsp;¬∑&nbsp;<span>2002 words</span>&nbsp;¬∑&nbsp;<span>Abhishek Benny</span></div></header><figure class=entry-cover><img loading=eager src=http://localhost:1313/images/covers/archiving_non-standard_experiments.png alt="Entropy moths munching on unorganized research data"></figure><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#why-does-knowledge-go-missing>Why does knowledge go missing?</a></li><li><a href=#defining-the-archive--minimum-viable-metadata>Defining the archive ‚Äì minimum viable metadata</a></li><li><a href=#the-square-peg--round-hole-fallacy--standardize-the-archiving-not-the-experiments>The square peg / round hole fallacy ‚Äì standardize the archiving, not the experiments</a></li><li><a href=#if-you-want-to-find-it-tag-it--improving-searchability>If you want to find it, tag it ‚Äì improving searchability</a></li><li><a href=#if-you-cant-trace-it-you-cant-trust-it--design-for-auditability>If you can‚Äôt trace it, you can‚Äôt trust it ‚Äì design for auditability</a></li><li><a href=#making-the-horse-drink--driving-adoption>Making the horse drink ‚Äì driving adoption</a></li><li><a href=#keeping-the-entropy-moths-out>Keeping the entropy moths out&mldr;</a></li></ul></nav></div></details></div><div class=post-content><p>In any R&amp;D or continuous improvement environment, progress depends on knowledge transfer. Often, a lot of the most valuable lessons live in people‚Äôs heads: the ‚Äúwe tried that in 2014 ‚Ä¶‚Äù experiments, the one-off protocols, the messy edge cases and details that did not make it into a formal report. Newer teams can try to survey the literature and internal documentation, but without a guide, it‚Äôs hard to know what exists, let alone where it‚Äôs stored, what to trust, and under what circumstances the data is valid.</p><p>When one of these human libraries leaves, the organization does not just lose their experience, it also loses the retrievability of the data. You‚Äôre stranded, repeating work, missing prior constraints, losing context and spending large amounts of time and money rebuilding it from scratch. And that‚Äôs when the entropy moths move in, quietly chewing through whatever scraps remain: half-labeled folders, orphaned spreadsheets, and undocumented assumptions, until ‚Äúwe definitely learned something here‚Äù becomes ‚Äúwe have no idea what this means.‚Äù</p><p>That‚Äôs why a well-maintained, searchable knowledgebase isn‚Äôt a ‚Äúnice to have‚Äù. It‚Äôs vital infrastructure. The catch is that continuous improvement is inherently ad hoc. Protocols vary, outputs are not standardized, and important metadata is often scattered across OneNote, meeting minutes, and oral tradition. Archiving this is non-trivial: the system needs to be searchable and auditable without stripping away the data richness that makes it worth keeping around.</p><p>This was a core problem that I was solving in my previous role at Bayer, and more recently with my own projects. I recently built a system to run systematic backtests across large parameter grids, which naturally needed a similar knowledgebase. In this post, I‚Äôll show how to moth-proof your experiments against entropy by building an archive that captures minimum viable metadata, enforces a consistent storage ‚Äúshape,‚Äù and keeps an auditable thread from figure to dataset to run to protocol.</p><h2 id=why-does-knowledge-go-missing>Why does knowledge go missing?<a hidden class=anchor aria-hidden=true href=#why-does-knowledge-go-missing>#</a></h2><p>Most archives don‚Äôt fail catastrophically; they get eroded slowly. Usually there are a few modes of failure:</p><ul><li>The test exists, but the context does not (why did we perform the test?)</li><li>The protocol exists, but metadata does not (what product did we test on? Who operated it? What version of automation did we do it with?)</li><li>Results exist, but the interpretation does not (were the results good? Why?)</li><li>Files existed on a person‚Äôs personal drive, but got deleted by a retention policy</li><li>Multiple floating versions of a slide deck or report (Atest_final_FINAL_v7.pptx)</li></ul><p>Sometimes we have combinations of these scenarios, where the context for a follow up protocol is lost because of shared drive, or there are fundamental changes to methods that are lost.</p><p>Now that the stakes are clear, let‚Äôs build an archive the entropy moths can‚Äôt eat!</p><h2 id=defining-the-archive--minimum-viable-metadata>Defining the archive ‚Äì minimum viable metadata<a hidden class=anchor aria-hidden=true href=#defining-the-archive--minimum-viable-metadata>#</a></h2><p>Unless we structure our thinking around protocols and testing, it is impossible to archive them.</p><p>Every protocol starts with a research question. It usually takes the form of: ‚ÄúHow can I modify X to get Y benefit from Z process?‚Äù Inherent to this question, we have a few of the KEY items we need to make our archive work. What we are testing? (modifying X), why are we doing it? (get y benefit), what is the context? (from Z process). While this is very simple, most testing follows this very pattern. We are already halfway there to defining the minimum viable metadata that we need to have a searchable archive. Here‚Äôs a quick reference of what this consists of.</p><p><strong>Table 1. Examples of minimum viable metadata and why we need them</strong></p><table><thead><tr><th style=text-align:left>Category</th><th style=text-align:left>Examples</th><th style=text-align:left>Purpose</th></tr></thead><tbody><tr><td style=text-align:left>Identity</td><td style=text-align:left>Protocol ID<br>Title<br>Owner<br>Date</td><td style=text-align:left>Identifies the protocol from similar ones to reduce confusion.</td></tr><tr><td style=text-align:left>Linkages</td><td style=text-align:left>Related testing<br>Followups<br>Tags</td><td style=text-align:left>Links tests together so that finding context is easier without needing to provide redundant data.</td></tr><tr><td style=text-align:left>Justification</td><td style=text-align:left>Hypothesis<br>Business case<br>Success criteria</td><td style=text-align:left>Gives context as to why the test was performed.</td></tr><tr><td style=text-align:left>Inputs</td><td style=text-align:left>Materials<br>Parameter space</td><td style=text-align:left>Gives detail on what was tested and important context for future design of experiments.</td></tr><tr><td style=text-align:left>Methods</td><td style=text-align:left>Operating procedures<br>Equipment¬†and¬†versions<br>Sampling methods<br>Assays conducted</td><td style=text-align:left>Gives context on what may have changed and if the results are still valid.</td></tr><tr><td style=text-align:left>Knowledge Generated</td><td style=text-align:left>Conclusions<br>Recommendations<br>Edge cases<br>Key discussion<br>Anticipated issues</td><td style=text-align:left>This is what we are trying to preserve.</td></tr></tbody></table><p>This data is best stored in a sql database, however you can make excel or sharepoint work depending on your organization‚Äôs preexisting technology stack.</p><h2 id=the-square-peg--round-hole-fallacy--standardize-the-archiving-not-the-experiments>The square peg / round hole fallacy ‚Äì standardize the archiving, not the experiments<a hidden class=anchor aria-hidden=true href=#the-square-peg--round-hole-fallacy--standardize-the-archiving-not-the-experiments>#</a></h2><p>Now that we have defined our non-negotiable data points which give our archive structure, you‚Äôre probably wondering how we are going to take all the rest of the data which usually has VERY LITTLE in common between tests and force them into the same database structure. The answer is ‚Äì you don‚Äôt.</p><p>You are going to standardize the envelope of the data rather than the content. What this means is you standardize how stuff is stored. Here‚Äôs an example file system based on the protocol identification.</p><pre tabindex=0><code>Archive/
  ‚îú‚îÄ 20260101_testing_x_y_z/
  ‚îÇ  ‚îú‚îÄ 20260101_protocol.docx
  ‚îÇ  ‚îú‚îÄ 20260101_runs.csv
  ‚îÇ  ‚îú‚îÄ 20260101_results.pptx
  ‚îÇ  ‚îú‚îÄ 20260101_data.xlsx
  ‚îÇ  ‚îî‚îÄ rawdata/
  ‚îÇ     ‚îú‚îÄ 20260101_assay1.xlsx
  ‚îÇ     ‚îú‚îÄ 20260101_assay2.xlsx
  ‚îÇ     ‚îî‚îÄ 20260101_statistics.ipynb
  ‚îú‚îÄ 20251201_testing_a_b_c/
  ‚îÇ  ‚îî‚îÄ ‚Ä¶
  ‚îî‚îÄ ‚Ä¶
</code></pre><p>This way, even if every test is different, the path to find the data is always the same.</p><p>Congratulations, if you implemented the last 2 steps, you have a functional albeit basic archive. The challenge now is improving searchability, making it auditable, and getting people to use it.</p><h2 id=if-you-want-to-find-it-tag-it--improving-searchability>If you want to find it, tag it ‚Äì improving searchability<a hidden class=anchor aria-hidden=true href=#if-you-want-to-find-it-tag-it--improving-searchability>#</a></h2><p>If you are familiar with SEO, you will know that good tagging is the cornerstone of making any content pop up in searches. The simplest system for tagging experiments is by using free tags.</p><p>Free tags are liberating in that there is no rigidity, it is easy to implement and as your areas of study expand, you can expand the tag space without any additional development time and work on the archive system. However, in a system where you may have multiple groups or individuals with their own lexicon for the same phenomena this has the potential to spiral into being useless.</p><p>On the opposite end of the spectrum, if you use only controlled vocabulary (a dropdown) with rigid validation, you need more data management and development resources to grow the archive to fit the growing research needs. Once again, making the tagging system useless without the development resources.</p><p>This calls for a hybrid solution, where you add a few tags that are required and validated fields in the metadata database. These are tags you know will likely be associated with all tests such as process, test type, material type ‚Äì with the option of being left as Not Listed. The remainder of the tags should be in free tag form to allow expansion of the system as needed.</p><p>Depending on the span of the knowledgebase, it may be worth reviewing experiments on a set time span (Annually or Semi-Annually) and making sure that tags align between departments and experimental groups.</p><h2 id=if-you-cant-trace-it-you-cant-trust-it--design-for-auditability>If you can‚Äôt trace it, you can‚Äôt trust it ‚Äì design for auditability<a hidden class=anchor aria-hidden=true href=#if-you-cant-trace-it-you-cant-trust-it--design-for-auditability>#</a></h2><p>Unless you trust the data‚Äôs integrity, you cannot trust any conclusions you draw from it. The notion of auditability is making sure that the data is traceable and reproducible. This is where good data management principles come into play.</p><p>Raw data needs to be immutable or at least dated and signed. Most lab notebooks allow for signing and dating raw data and observations. In excel forms, it is best practice to include a date and signature at the bottom of the data file.</p><p>All the data included in figures need to be traceable: figure -> dataset -> run -> protocol. This allows reproduction of statistics and reuse of the data in comparison or meta studies. This is especially useful when tracking BOM (bill of materials) changes and changes in product quality ‚Äì as you can iteratively look at deviations in quality metrics with each BOM change experiment.</p><p>All deviations from protocol need to be recorded. Anything from as impactful as ‚ÄúWe excluded runs x-y because data from previous runs shows no issues‚Äù to ‚Äúwe used the wrong sample ID‚Äù need to be recorded. These deviations may be overlooked in the future leading to wrong or misleading analysis in meta studies.</p><p>Protocols need to be versioned. Like deviations from the protocol, changes to the protocol need to be recorded. Ensure that the protocol placed in the archive is the one that was actually followed.</p><p>Assay methods and analysis scripts need to be versioned. Changes to analysis scripts, statistical methods, or assay methodology needs to be documented and versioned so that there is context on the results. Not having documentation here can lead to misleading comparisons in future studies.</p><p>These rules are best enforced at the time of protocol archiving ‚Äì that is right after the study is concluded and results are published. If you wait, details fade and the moths start nibbling at your context.</p><h2 id=making-the-horse-drink--driving-adoption>Making the horse drink ‚Äì driving adoption<a hidden class=anchor aria-hidden=true href=#making-the-horse-drink--driving-adoption>#</a></h2><p>You can bring the horse to the river but you cannot force it to drink. The most challenging part of designing any system is getting people to use it. If you followed along to this point, we are left with a very robust archive system that is very searchable and maintains good data management principles. The downside of being robust is that it is heavy on additional work.</p><p>The best way to reduce friction and get your research teams to use it is to add in a good user interface. This is twofold, both the search functionality and the data entry methods need to be optimized.</p><p>Arguably, the more important of the two is the search function. All of the tooling you built for tagging and metadata are useless unless your search engine can parse through the data and retrieve relevant results. This can be as simple as the search/filter tooling on Sharepoint or a quick webapp, or as complicated as using an LLM trained on the data.</p><p>And now the counterargument: there won‚Äôt be any data to be searched if you can‚Äôt get your teams to input it. You need to develop a good UI that allows you to have validated dropdowns and defaults keyed in to reduce time spent entering data. This can be as simple as a quick PowerApp or Form which gives a few dropdowns and has a few defaults in there to speed up the entry process. Once again, depending on resources, you could outsource this process to an LLM, or at least the tagging.</p><p><strong>WORD OF CAUTION:</strong> LLMs enhance discoverability and ease of tagging; but does not substitute for good data management.</p><h2 id=keeping-the-entropy-moths-out>Keeping the entropy moths out&mldr;<a hidden class=anchor aria-hidden=true href=#keeping-the-entropy-moths-out>#</a></h2><p>A good archive does not try to turn every experiment into a standardized form. It does something far simpler and significantly more powerful: it makes every experiment findable, traceable, and reuseable. By capturing the minimum viable metadata, enforcing a consistent storage method, and layering in a hybrid tagging system, you build a knowledgebase that survives team changes, retention policies and the slow erosion of context. Auditability closes the loop by ensuring that results can always be walked backward: from a figure to the protocol that generated them so future comparisons don‚Äôt become mythology.</p><p>The last challenge is the most human one: adoption. The system needs to feel like a shortcut to research, not a chore. When the search works well, data entry is fast and when teams can reliably reuse past work your archive transforms from being just documentation and starts being a compounding asset.</p><p><strong>Moth-proofed at last.</strong></p><p><small><strong>genAI declaration:</strong> The text matter of this blog post was completed without the use of generative AI tools. Cover image created using OpenAI‚Äôs image generation tool (DALL¬∑E). Prompt: Library of scrolls with a jumble of papers on table that are being eaten by psychadelic colored moths. Time distortion effect in the background. Semi-messy composition, blog cover illustration, landscape (1920x1080). - Abhishek Benny.</small></p></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=next href=http://localhost:1313/stacks/presenting_high-dimensional_data/><span class=title>Next ¬ª</span><br><span>Presenting High-Dimensional Data (Without Summoning Eldritch Horrorsü¶ë)</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Archiving Non-Standard Experiments (Entropy-Moth Proofing) on x" href="https://x.com/intent/tweet/?text=Archiving%20Non-Standard%20Experiments%20%28Entropy-Moth%20Proofing%29&amp;url=http%3a%2f%2flocalhost%3a1313%2fstacks%2farchiving_non-standard_experiments%2f&amp;hashtags="><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Archiving Non-Standard Experiments (Entropy-Moth Proofing) on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fstacks%2farchiving_non-standard_experiments%2f&amp;title=Archiving%20Non-Standard%20Experiments%20%28Entropy-Moth%20Proofing%29&amp;summary=Archiving%20Non-Standard%20Experiments%20%28Entropy-Moth%20Proofing%29&amp;source=http%3a%2f%2flocalhost%3a1313%2fstacks%2farchiving_non-standard_experiments%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Archiving Non-Standard Experiments (Entropy-Moth Proofing) on reddit" href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fstacks%2farchiving_non-standard_experiments%2f&title=Archiving%20Non-Standard%20Experiments%20%28Entropy-Moth%20Proofing%29"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Archiving Non-Standard Experiments (Entropy-Moth Proofing) on facebook" href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fstacks%2farchiving_non-standard_experiments%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Archiving Non-Standard Experiments (Entropy-Moth Proofing) on whatsapp" href="https://api.whatsapp.com/send?text=Archiving%20Non-Standard%20Experiments%20%28Entropy-Moth%20Proofing%29%20-%20http%3a%2f%2flocalhost%3a1313%2fstacks%2farchiving_non-standard_experiments%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Archiving Non-Standard Experiments (Entropy-Moth Proofing) on telegram" href="https://telegram.me/share/url?text=Archiving%20Non-Standard%20Experiments%20%28Entropy-Moth%20Proofing%29&amp;url=http%3a%2f%2flocalhost%3a1313%2fstacks%2farchiving_non-standard_experiments%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Archiving Non-Standard Experiments (Entropy-Moth Proofing) on ycombinator" href="https://news.ycombinator.com/submitlink?t=Archiving%20Non-Standard%20Experiments%20%28Entropy-Moth%20Proofing%29&u=http%3a%2f%2flocalhost%3a1313%2fstacks%2farchiving_non-standard_experiments%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2026 <a href=http://localhost:1313/>Stacks of Scrolls</a></span> ¬∑
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script></body></html>